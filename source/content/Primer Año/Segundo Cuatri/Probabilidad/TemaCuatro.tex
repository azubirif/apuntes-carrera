%! TEX root = Probabilidad.tex

\documentclass{./Probabilidad.tex}

\begin{document}
\chapter{Tema Cuatro}
\section{Variable aleatoria}
Existen varias formas de clasificar variables:
\begin{itemize}
	\item Cualitativa: en función de una cualidad
	\item Cuantitativa: se puede medir y evaluar con un número.
	\item Cuasi-cuantitativa: se ordenan en función de una magnitud que puede pertenecer a diferentes intervalos.
\end{itemize}
Dado un modelo matemático $(\Omega, \mathcal{A}, P)$ formada por un espacio muestral $\Omega$, un álgebra de sucesos $\mathcal{A}$, y una probabilidad $P$.
Una variable discreta $X$ definida sobre el modelo es una función definida como
$$
X:\Omega\to \mathbb{R}
$$
Que se denomina como función de masa $X_{i}$.\\
Se llama función de distribución de $X$ a la función $F(X)$ que
$$
F(X=x_{j})=P(\{ \omega \in\Omega:X(\omega)\leq x_{j} \})=P(-\infty,x_{j}]
$$
\begin{itemize}
	\item $\lim_{ x \to -\infty }F(X)=0$
	\item $\lim_{ x \to \infty }F(X)=1$
	\item $F$ es monótona decreciente.
	\item $F$ es continua por la derecha.
	\item $P(x_{1}< x\leq x_{2})=F(x_{2})-F(x_{1})$
\end{itemize}
\section{Estadística descriptiva}
Dada una variable aleatoria discreta $X$, se define esperanza de $X$ como
$$
\mu=E(X)=\sum_{i}x_{i}P(X=x_{i})
$$
también la moda, que se define como el valor con mayor probabilidad.\\
La mediana se define como el valor que se encuentra en la mitad del resto de los valores:
$$
P(-\infty, med]=F(X=med) \geq \frac{1}{2}
$$
\pagebreak
\subsection{Distribución de una variable condicionada por un suceso $A$}
Dada una función de masa $P(X)$, la probabilidad de que $X$ tome $x_{i}$ si ha ocurrido $A$ viene dada por
$$
P(X=x_{i}|A)=\frac{P(\{ \omega \in \Omega|X(\omega)=x_{i} \}\cap A)}{P(A)}
$$
y entonces
$$
E(X|A)=\sum x_{i}P(x_{i}|A)
$$
\section{Momentos respecto al origen}
Se denomina momento respecto al origen de orden $r \forall r \in \mathbb{N}$ y se denota como $\alpha_{r}$:
$$
\alpha_{r}=E[X^{r}] = \sum_{i=0}^{n} x_{i}^{r}P(X=x_{i})
$$
El momento de orden $1$ es el valor medio. Con esto podemos definir la varianza como
$$
\sigma^{2}=E[X^{2}]-E[X]^{2}
$$
También definimos
$$
\mu_{r} = \sum(x-\bar{x})^rP(x)
$$
\section{Entropía}
Definimos la entropía asociada a una variable aleatoria $X$ como
$$
H(X)= -\sum p(x_{i}) \log[p(x_{i})]
$$
\section{Distribuciones de probabilidad más comunes}
\subsection{Distribución binomial}
La función masa es
\[
	P(x) = P_{n}^{x,n-x}p^{x}(1-p)^{n-x}
\]
Y el valor medio es
\[
	E[X] = n\cdot p
\]
la varianza
\[
	\sigma ^{2}= np(1-p)
\]
\subsection{Distribución de Poisson}
Si $n \to \infty$ y $x \in [0, \infty)$, entonces la función masa binomial se aproxima a una función llamada \textbf{masa de Poisson}:
\[
	\lim_{(n,p) \to (\infty,0)} b(x,n,p)=p(x,\lambda) 
\]
donde $\lambda = n\cdot p$, y la función masa es
\[
	p(x,\lambda) = \left\{
		\begin{matrix}
			&\frac{e^{-\lambda} \lambda^{x}}{x!}~si~x=0,1,\dots ,n\\
			&0~else
	\end{matrix}\right\}
\]
Para $X\sim P(X,\lambda)$, la función distibución de $X$ se denota por
\[
	P(X;\lambda)=P \{ \omega \in \Omega : X \leq x\} = \sum_{y=0}^{x} p(y,\lambda)
\]
Donde se cumple que $\mu = E(X) = \lambda$, y que $V(X) = \sigma ^{2} = \lambda$ 
\end{document}
