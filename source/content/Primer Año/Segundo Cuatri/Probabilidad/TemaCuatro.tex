%! TEX root = Probabilidad.tex

\documentclass{./Probabilidad.tex}

\begin{document}
\chapter{Tema Cuatro}
\section{Variable aleatoria}
Existen varias formas de clasificar variables:
\begin{itemize}
	\item Cualitativa: en función de una cualidad
	\item Cuantitativa: se puede medir y evaluar con un número.
	\item Cuasi-cuantitativa: se ordenan en función de una magnitud que puede pertenecer a diferentes intervalos.
\end{itemize}
Dado un modelo matemático $(\Omega, \mathcal{A}, P)$ formada por un espacio muestral $\Omega$, un álgebra de sucesos $\mathcal{A}$, y una probabilidad $P$.
Una variable discreta $X$ definida sobre el modelo es una función definida como
$$
X:\Omega\to \mathbb{R}
$$
Que se denomina como función de masa $X_{i}$.\\
Se llama función de distribución de $X$ a la función $F(X)$ que
$$
F(X=x_{j})=P(\{ \omega \in\Omega:X(\omega)\leq x_{j} \})=P(-\infty,x_{j}]
$$
\begin{itemize}
	\item $\lim_{ x \to -\infty }F(X)=0$
	\item $\lim_{ x \to \infty }F(X)=1$
	\item $F$ es monótona decreciente.
	\item $F$ es continua por la derecha.
	\item $P(x_{1}< x\leq x_{2})=F(x_{2})-F(x_{1})$
\end{itemize}
\section{Estadística descriptiva}
Dada una variable aleatoria discreta $X$, se define esperanza de $X$ como
$$
\mu=E(X)=\sum_{i}x_{i}P(X=x_{i})
$$
también la moda, que se define como el valor con mayor probabilidad.\\
La mediana se define como el valor que se encuentra en la mitad del resto de los valores:
$$
P(-\infty, med]=F(X=med) \geq \frac{1}{2}
$$
\pagebreak
\subsection{Distribución de una variable condicionada por un suceso $A$}
Dada una función de masa $P(X)$, la probabilidad de que $X$ tome $x_{i}$ si ha ocurrido $A$ viene dada por
$$
P(X=x_{i}|A)=\frac{P(\{ \omega \in \Omega|X(\omega)=x_{i} \}\cap A)}{P(A)}
$$
y entonces
$$
E(X|A)=\sum x_{i}P(x_{i}|A)
$$
\section{Momentos respecto al origen}
Se denomina momento respecto al origen de orden $r \forall r \in \mathbb{N}$ y se denota como $\alpha_{r}$:
$$
\alpha_{r}=E[X^{r}] = \sum_{i=0}^{n} x_{i}^{r}P(X=x_{i})
$$
El momento de orden $1$ es el valor medio. Con esto podemos definir la varianza como
$$
\sigma^{2}=E[X^{2}]-E[X]^{2}
$$
También definimos
$$
\mu_{r} = \sum(x-\bar{x})^rP(x)
$$
\section{Entropía}
Definimos la entropía asociada a una variable aleatoria $X$ como
$$
H(X)= -\sum p(x_{i}) \log[p(x_{i})]
$$
\section{Distribuciones de probabilidad más comunes}
\subsection{Distribución binomial}
La función masa es
\[
	P(x) = P_{n}^{x,n-x}p^{x}(1-p)^{n-x}
\]
Y el valor medio es
\[
	E[X] = n\cdot p
\]
la varianza
\[
	\sigma ^{2}= np(1-p)
\]
\subsection{Distribución de Poisson}
Si $n \to \infty$ y $x \in [0, \infty)$, entonces la función masa binomial se aproxima a una función llamada \textbf{masa de Poisson}:
\[
	\lim_{(n,p) \to (\infty,0)} b(x,n,p)=p(x,\lambda) 
\]
donde $\lambda = n\cdot p$, y la función masa es
\[
	p(x,\lambda) = \left\{
		\begin{matrix}
			&\frac{e^{-\lambda} \lambda^{x}}{x!}~si~x=0,1,\dots ,n\\
			&0~else
	\end{matrix}\right\}
\]
Para $X\sim P(X,\lambda)$, la función distibución de $X$ se denota por
\[
	P(X;\lambda)=P \{ \omega \in \Omega : X \leq x\} = \sum_{y=0}^{x} p(y,\lambda)
\]
Donde se cumple que $\mu = E(X) = \lambda$, y que $V(X) = \sigma ^{2} = \lambda$ 
\subsection{Distribución geométrica}
Definimos la probabilidad de que $X=k$ como
\[
	P(X=k)=(1-p)^{k-1}p~si~k=1,2,3\dots 
\]
y la función distribución como
\[
	F(X=k)=P(-\infty,X]= \sum_{j=1}^{k} (1-p)^{j-1}p~si~x=0,1,\dots ,n
\]
teniendo que $\mu=E(X)=\frac{1}{p}$, y que $V(X)=\sigma ^{2}= \frac{1-p}{p ^{2}}$
\section{Distribución hipergeométrica}
Tenemos las siguientes suposiciones:
\begin{itemize}
	\item La población es finita y consiste de $N$ elementos.
	\item Cada individuo se caracteriza como un éxito $S$ o fracaso $F$, y hay $M$ éxitos.
	\item Se elige una muestra de $n$ individuos tal que cada subconjunto tenga las mismas probabilidades de ser elegido. 
\end{itemize}
Si $X$ es el número de éxitos, entedremos $M$ éxitos y $(N-M)$ fracasos:
\[
	P(X=x)=h(x;n,M,N)
\]
Además, se da que
\begin{itemize}
	\item $E(X)=n \frac{M}{N}$
	\item $V(X)=n \frac{N-n}{N-1} \frac{M}{N} (1 - \frac{M}{N})$ 
\end{itemize}
\textbf{Ejemplo}:\\
De $5$ animales de una especie en extinción, se toman $25$ animales y se liberan con otros individuos. Después de mezclarse, se tomaron $10$ de esos $25$ animales. Se define $X$ como el número de animales marcados en la segunda muestra que contiene $10$ animales. Indica la función masa de $X$ y ciertas probabilidades.\\
Datos:
\begin{itemize}
	\item $M=5$
	\item $N=25$
	\item $n=10$
	\item $x$ será el número de ejemplares marcados. 
\end{itemize}
Ahora nos interesa que $x=2$, por lo que aplicamos la ley de Laplace:
\[
	P(X=2)= \frac{\binom{5}{2}\binom{20}{8}}{\binom{25}{10}}\approx 0.38
\]
Para $x\leq 2$, tenemos que
\[
	P(X\leq 2) = P(X=0) + P(X=1)+P(X=2)=0.056+0.2569+0.385=0.6988
\]
Y la esperanza es $E(X)=2$.
\end{document}
